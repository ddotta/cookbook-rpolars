## Final results

### Performance

So what can we conclude?  
With which file format and which method is it fastest to execute the same request?  

Let's have a look! First, let's aggregate all the benchmark results:

```{r}
#| label: final-results-rbind

# Aggregation
bmk_results <- rbind(
  csv_bmk,
  unique_parquet_bmk,
  partitioned_parquet_bmk,
  duckdb_bmk
)
```

Let's do a few basic data manipulation before plotting the results :

```{r}
#| label: data-manipulation-before-plotting
#| code-fold: true
#| message: false
#| warning: false
#| results: 'hide'

# Sort the results
bmk_results$expr <- reorder(bmk_results$expr, bmk_results$time, decreasing = TRUE)

# Define a function to get different colors
get_label_color <- function(label) {
  if (endsWith(label, "partitioned parquet file")) {
    "red"
  } else if (endsWith(label, "unique parquet file")) {
    "orange"
  } else if (endsWith(label, "csv file")) {
    "blue"
  } else if (endsWith(label, "duckdb file")) {
    "green"
  } else {
    "black" 
  }
}
# Define colors for the plot in bmk_results
bmk_results$mycolor <- sapply(as.character(bmk_results$expr),get_label_color)
```


```{r}
#| label: final-results-plot
#| echo: false

autoplot(bmk_results) +
  aes(color = expr) +
  scale_color_manual(
    values = setNames(
      bmk_results$mycolor,
      as.character(bmk_results$expr)
    )
  ) +
  labs(title = "polars âš”ï¸ others : Benchmark results!",
       subtitle = "ğŸ” Right-click on the plot to open it wide in a new tab") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none")
```

::: {.callout-important title="Final conclusion"}

The first thing to note is that execution time is faster when working with **parquet files** than with csv files. ğŸ†  
**With the exception of polars in lazy mode, execution time is systematically lower with parquet files.**

The big winner of our benchmark is nevertheless **polars (lazy mode)**, which takes the top 2 places in our ranking. ğŸ†ğŸ†ğŸ†  
 
This is followed by processing with arrow and then with SQL.   
 
Not surprisingly, R base and dplyr are at the bottom of the ranking.
:::

### Memory usage

We've just analysed the performance of the various alternatives to Polars, but what about R's memory usage?  

To do this, we're going to use `mem_change()` from {pryr} package. This method tells you how memory changes during code execution. Positive numbers represent an increase in the memory used by R, and negative numbers represent a decrease. 


```{r}
#| label: memory-usage-results
#| echo: false

gc()

memory_usage_df <- data.frame(
  functions = c("csv_rbase()","csv_dplyr()","csv_dt()",
                "csv_eager_polars()","csv_arrow",
                "arrow_lazy |> collect()","arrow_eager()",
                "parquet_duckdb_sql()","parquet_polars_lazy()$collect()$to_data_frame()",
                "partitioned_parquet_arrow_lazy()","partitioned_parquet_dplyr_duckdb()",
                "partitioned_parquet_polars_lazy()","parquet_polars_lazy()$collect()",
                "parquet_polars_lazy()$collect()$to_data_frame()"),
  change = c(mem_change(csv_rbase()),mem_change(csv_dplyr()),mem_change(csv_dt()),
             mem_change(csv_eager_polars()),mem_change(csv_arrow()),
             mem_change(arrow_lazy() |> collect()),mem_change(arrow_eager()),
             mem_change(parquet_duckdb_sql()),mem_change(parquet_polars_lazy()$collect()$to_data_frame()),
             mem_change(partitioned_parquet_arrow_lazy()),mem_change(partitioned_parquet_dplyr_duckdb()),
             mem_change(partitioned_parquet_polars_lazy()),mem_change(parquet_polars_lazy()$collect()),
             mem_change(parquet_polars_lazy()$collect()$to_data_frame()))
) |> arrange(desc(change))

# CrÃ©ation du graphique avec ggplot2
ggplot(memory_usage_df, aes(x = change, y = functions)) +
  geom_col(fill = "blue") +
  labs(title = "Memory consumption according to functions used", x = "Memory consumption", y = "Functions")

```

::: {.callout-important title="Final conclusions"}

ğŸ‘‰ **A few conclusions can be drawn from this section on benchmarking:**  

:one: It is **more efficient** to work **from a parquet or duckdb file**;
:two: In terms of **execution speed**, there is **no great difference between a single parquet file and several partitioned parquet files** (although the gap will undoubtedly widen in favour of partitioned files if the size of the initial work file is increased);
:three: **Lazy evaluation of polars performs best, followed by SQL queries executed directly on a duckdb file.** ğŸ†ğŸ†ğŸ† 

:::