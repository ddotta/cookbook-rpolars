## Final results

### Performance

So what can we conclude?  
With which file format and which method is it fastest to execute the same request?  

Let's have a look! First, let's aggregate all the benchmark results:

```{r}
#| label: final-results-rbind

# Aggregation
bmk_results <- rbind(
  csv_bmk,
  unique_parquet_bmk,
  partitioned_parquet_bmk,
  duckdb_bmk
)
```

Let's do a quick sort on the `expr` column before plotting the results :

```{r}
#| label: data-manipulation-before-plotting
#| code-fold: true
#| message: false
#| warning: false
#| results: 'hide'

# Sort the results
bmk_results$expr <- reorder(bmk_results$expr, bmk_results$time, decreasing = TRUE)
```


```{r}
#| label: final-results-plot
#| echo: false
#| warning: false

speed_plot <- autoplot(bmk_results) +
  labs(title = "polars ‚öîÔ∏è others : Benchmark results!",
       subtitle = "üîé Right-click on the plot to open it wide in a new tab") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none") +
  theme(axis.text.y = element_text(color = c("orange","orange","red","orange",
                                             "orange","orange","red","blue",
                                             "blue","blue","green","orange","red","blue"))) +
  aes(color = expr) +
  scale_color_manual('Functions', values = c("orange","orange","red","orange",
                                             "orange","orange","red","blue",
                                             "blue","blue","green","orange","red","blue"))
speed_plot
```

::: {.callout-important title="Final conclusions" icon=false}

üëâ **A few conclusions can be drawn from this section on benchmarking:**  

- It is **more efficient** to work **from a parquet or duckdb file** except for **polars with lazy evaluation which is very fast**;  
- In terms of **execution speed**, there is **no great difference between a single parquet file and several partitioned parquet files** (although the gap will undoubtedly widen in favour of partitioned files if the size of the initial work file is increased);  
- **Lazy evaluation of polars performs best whatever the format of the file you are working on.** üèÜüèÜ  
It's followed by **SQL queries** executed directly on a **duckdb file.** üèÜ   

:::

### Memory usage

We've just analysed the performance of the various alternatives to Polars, but what about R's memory usage?  

To do this, we're going to use `mem_change()` from `{pryr}` package. This method tells you how memory changes during code execution. Positive numbers represent an increase in the memory used by R, and negative numbers represent a decrease. 

```{r}
#| label: garbage-collection
#| echo: false
#| results: 'hide'
gc()
```

```{r}
#| label: memory-usage-results
#| echo: false
#| warning: false

memory_usage_df <- data.frame(
  functions = c("polars (lazy) - from unique parquet file",
                "polars (lazy) - from partitioned parquet file",
                "polars (lazy) - from csv file",
                "Duckdb and SQL - from unique parquet file",
                "arrow (eager) - from unique parquet file",
                "arrow (lazy) - from unique parquet file",
                "arrow (lazy) - from partitioned parquet file",
                "SQL from duckdb file",
                "data.table - from csv file",
                "polars (eager) from csv file",
                "dplyr (Acero) - from csv file",
                "dplyr (duckdb) - from partitioned parquet file",
                "dplyr - from csv file",
                "R base - from csv file"),
  change = c(mem_change(as.data.frame(parquet_polars_lazy()$collect())),
             mem_change(as.data.frame(partitioned_parquet_polars_lazy())),
             mem_change(csv_lazy_polars()$collect()),
             mem_change(duckdb_dbfile_sql()),
             mem_change(arrow_eager()),
             mem_change(arrow_lazy() |> collect()),
             mem_change(partitioned_parquet_arrow_lazy()),
             mem_change(parquet_duckdb_sql()),
             mem_change(csv_dt()),
             mem_change(csv_eager_polars()),
             mem_change(csv_arrow()),
             mem_change(partitioned_parquet_dplyr_duckdb()),
             mem_change(csv_dplyr()),
             mem_change(csv_rbase()))
)

  
# Cr√©ation du graphique avec ggplot2
mem_plot <- ggplot(memory_usage_df, 
                   aes(x = change,
                       y = fct_relevel(
                         functions,
                         "R base - from csv file",
                         "dplyr - from csv file",                         
                         "dplyr (duckdb) - from partitioned parquet file",
                         "dplyr (Acero) - from csv file",                 
                         "polars (eager) from csv file" ,                 
                         "data.table - from csv file",                    
                         "SQL from duckdb file",      
                         "arrow (lazy) - from partitioned parquet file", 
                         "arrow (lazy) - from unique parquet file",     
                         "arrow (eager) - from unique parquet file",      
                         "Duckdb and SQL - from unique parquet file",     
                         "polars (lazy) - from csv file",
                         "polars (lazy) - from partitioned parquet file" ,
                         "polars (lazy) - from unique parquet file"))) +
  geom_col(fill = rev(c("orange","orange","red","orange",
                            "orange","orange","green","red","blue",
                            "blue","blue","orange","red","blue"))) +
                              labs(title = "Memory consumption according to functions used",
                                   subtitle = "üîé Right-click on the plot to open it wide in a new tab",
                                   x = "Memory consumption", y = "Functions") +
    theme(axis.text.y = element_text(color = c("orange","orange","red","orange",
                                               "orange","orange","green","red","blue",
                                               "blue","blue","orange","red","blue")))
mem_plot
```

::: {.callout-important title="Memory usage conclusions" icon=false}

üëâ **A few conclusions can be drawn from this section on benchmarking about memory usage:**  

- Firstly, the method with `data.table from a csv file` surprisingly **consumes a lot of RAM**. Maybe it's related to the `as.data.table()` conversion? If a reader has an explanation, I'm interested and feel free to open an issue;   
- Regarding csv files, syntaxes with `R base` and `dplyr` are the **least consuming RAM** (but at the expense of speed);  
- Regarding parquet files, syntaxes with `arrow (eager)` and `Duckdb with SQL` are the **least consuming RAM**;  
- The `SQL language used on a Duckdb file` also consumes **very little RAM**  

üèÜüèÜüèÜ   

:::