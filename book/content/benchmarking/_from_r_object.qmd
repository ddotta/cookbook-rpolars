## From an R object

This section analyses the different methods for making a query **from an R object already loaded in memory**.

Let's start by comparing polars with R base, dplyr and data.table.

::: {.panel-tabset}

## polars

```{r}
#| label: rboject-polars-benchmarking

robject_polars <- function() {
  
  pl$DataFrame(DataMultiTypes)$
    # Filter rows
    filter(
      pl$col("colInt")>2000 & pl$col("colInt")<8000
    )$
    # Grouping and aggregation
    groupby(
      "colString")$
    agg(
      pl$col("colInt")$min()$alias("min_colInt"),
      pl$col("colInt")$mean()$alias("mean_colInt"),
      pl$col("colInt")$max()$alias("max_colInt"),
      pl$col("colNum")$min()$alias("min_colNum"),
      pl$col("colNum")$mean()$alias("mean_colNum"),
      pl$col("colNum")$max()$alias("max_colNum")
    )
}
```

## R base

```{r}
#| label: rboject-rbase-benchmarking

robject_rbase <- function() {
  
  # Grouping and aggregation from data filtered
  aggregate(cbind(colInt, colNum) ~ colString, 
            data = DataMultiTypes[DataMultiTypes$colInt>2000 & DataMultiTypes$colInt<8000,], 
            FUN = function(x) c(mean = mean(x), 
                                min = min(x), 
                                max = max(x)))
  
}
```

## dplyr

```{r}
#| label: rboject-dplyr-benchmarking

robject_dplyr <- function() {
  
  DataMultiTypes |>
    
   # Filter rows
    filter(
      colInt>2000 & colInt<8000
      ) |>
    
    # Grouping and aggregation
    group_by(colString) |> 
    
    summarise(
      min_colInt = min(colInt),
      mean_colInt = mean(colInt),
      mas_colInt = max(colInt),
      min_colNum = min(colNum),
      mean_colNum = mean(colNum),
      max_colNum = max(colNum)
  )

}
```

## data.table

```{r}
#| label: rboject-datatable-benchmarking
robject_dt <- function() {
  
  as.data.table(DataMultiTypes)[
    
    colInt > 2000 & colInt < 8000
    
  ][, .(min_colInt = min(colInt),
        mean_colInt = mean(colInt),
        mas_colInt = max(colInt),
        min_colNum = min(colNum),
        mean_colNum = mean(colNum),
        max_colNum = max(colNum)),
    
    by = colString
  ]
}
```
:::

Now let's look at how to use the **DuckDb engine** on R objects.  
There are **3 main possibilities**:  

1. To use the DuckDB engine to query a R object with **dplyr**, you can use the `duckdb::duckdb_register()` method and then the `dplyr::tbl()` method to pass your dplyr instructions (**dplyr/DuckDB**). 

2. To use the DuckDB engine to query a R object with **the standard DBI methods**, you can use the `duckdb::duckdb_register()` method and then the `DBI::dbGetQuery()` method to pass your SQL query (**SQL/DuckDB**).

3. To use the DuckDB engine to query a R object in combination **with {arrow} package**, you can use the `arrow::to_duckdb()` and then pass your dplyr instructions (**dplyr/arrow/DuckDB**).

::: {.panel-tabset}

## dplyr/DuckDB

```{r}
#| label: robject-duckdb-dplyr-benchmarking

robject_duckdb_dplyr <- function(variables) {
  
  con <- DBI::dbConnect(duckdb::duckdb())

  duckdb::duckdb_register(con, "DataMultiTypes", DataMultiTypes)

  tbl(con, "DataMultiTypes") |>
    
    # Filter rows
    filter(
      colInt>2000 & colInt<8000
    ) |>
    # Grouping and aggregation
    group_by(colString) |> 
    summarise(
      min_colInt = min(colInt, na.rm = TRUE),
      mean_colInt = mean(colInt, na.rm = TRUE),
      mas_colInt = max(colInt, na.rm = TRUE),
      min_colNum = min(colNum, na.rm = TRUE),
      mean_colNum = mean(colNum, na.rm = TRUE),
      max_colNum = max(colNum, na.rm = TRUE)
    ) |>
    collect()
  
  DBI::dbDisconnect(con, shutdown=TRUE)
  
}
```

## SQL/DuckDB

```{r}
#| label: robject-duckdb-sql-benchmarking

robject_duckdb_sql <- function(variables) {
  
  con <- DBI::dbConnect(duckdb::duckdb())

  duckdb::duckdb_register(con, "DataMultiTypes", DataMultiTypes)

  DBI::dbGetQuery(
    con, 
    "SELECT colString,
           MIN(colInt) AS min_colInt,
           AVG(colInt) AS mean_colInt,
           MAX(colInt) AS max_colInt,
           MIN(colNum) AS min_colNum,
           AVG(colNum) AS mean_colNum,
           MAX(colNum) AS max_colNum
    FROM (
        SELECT colString,
               colInt,
               colNum
        FROM DataMultiTypes
        WHERE colInt > 2000 AND colInt < 8000
) AS filtered_data
GROUP BY colString;")
  
  DBI::dbDisconnect(con, shutdown=TRUE)
  
}
```

## dplyr/arrow/DuckDB

```{r}
#| label: robject-duckdb-dplyr-arrow-benchmarking

robject_duckdb_arrow_dplyr <- function(variables) {
          
  DataMultiTypes |>
    
    to_duckdb() |>
    
    # Filter rows
    filter(
      colInt>2000 & colInt<8000
    ) |>
    # Grouping and aggregation
    group_by(colString) |> 
    
    summarise(
      min_colInt = min(colInt, na.rm = TRUE),
      mean_colInt = mean(colInt, na.rm = TRUE),
      mas_colInt = max(colInt, na.rm = TRUE),
      min_colNum = min(colNum, na.rm = TRUE),
      mean_colNum = mean(colNum, na.rm = TRUE),
      max_colNum = max(colNum, na.rm = TRUE)
    ) 
  
}
```
:::

::: {.callout-tip}
One of the advantages of using the DuckDB engine and dplyr may be **to use a feature implemented by DuckDB but not yet by Arrow**. We can do the opposite, and return to the Arrow engine with `arrow::to_arrow()`.

However, the benchmark results are clear: SQL queries are by far the fastest! üèÜ
:::


### Results with a R object

```{r}
#| label: from-robject-results-benchmarking
#| message: false
#| warning: false

microbenchmark(
  robject_polars(),
  robject_rbase(),
  robject_dplyr(),
  robject_dt(),
  robject_duckdb_dplyr(),
  robject_duckdb_sql(),
  robject_duckdb_arrow_dplyr(),
  times = 5
 )
```
