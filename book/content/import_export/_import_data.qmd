## Import data

### Read a csv file or URL

The `read_csv()` method can be used to import a csv file from a file or an URL.
`read_csv()` returns a `DataFrame`.  
Its main arguments are:  

- `path`: path to a file or URL.  
- `sep`: single character to use as delimiter in the csv file.  
- `ignore_errors`: boolean. Indicate if the first row of dataset is a header or not. If set to `FALSE`, column names will be autogenerated in the following format: column_x, with x being an enumeration over every column in the dataset starting at 1.  
- `skip_rows`: integer. Start reading after skip_rows lines. The header will be parsed at this offset.  
- `n_rows`: integer. Stop reading after reading n_rows.  
- `cache`: boolean. Cache the result after reading.  
- `overwrite_dtype`: named list of dtypes where name points to a column. Can overwrite dtypes during inference.  
- `low_memory`: boolean. Reduce memory usage in expense of performance.  
- `comment_char`: single byte character used for csv quoting, default = ". Set to NA to turn off special handling and escaping of quotes.  
- `null_values`: values to interpret as null values.  
- `infer_schem_length`: maximum number of rows to read to infer the column types. If set to 0, all columns will be read as UTF-8. If NULL, a full table scan will be done (slow).  
- `skip_rows_after_header`: boolean. Skip this number of rows when the header is parsed.  
- `encoding`: either "utf8" or "utf8-lossy". Lossy means that invalid utf8 values are replaced with "?" characters.  
- `row_count_name`: string. Name of a added row count column.  
- `row_count_offset`: integer. Offset to start the row_count column (only used if the name is set).  
- `parse_dates`: boolean. Try to automatically parse dates. If this does not succeed, the column remains of data type `Utf8`.  
- `reuse_downloaded`: boolean. If TRUE(default) and a URL was provided, cache the downloaded files in session for an easy reuse.

By default, polars takes the first row of the csv file as the header to set column names. If the first row is not a header, you can set the argument `has_header = FALSE` and the column names will be `column_1`, `column_2`...

#### From a file

::: {.panel-tabset} 

## polars

```{r}
#| label: read_csv-from-file-polars
pl$read_csv("examples/iris.csv")
```

## R base

```{r}
#| label: read-csv-rbase
read.csv("examples/iris.csv")
```
:::

#### From multiple files

First, let's create a dozen csv files

```{r}
#| label: creation-ten-csv-files
#| message: false
#| warning: false
dir.create("Datasets")
mydf <- data.frame(
  col1 = 1:3,
  col2 = c("a", "b", "c")
)
for (i in 1:10) {
  write.csv(mydf, file = paste0("Datasets/example_data_",i,".csv"))
}
```

::: {.callout-important}
June 2023: Reading those multiple files into a single `DataFrame` is not yet implemented in R. See [here](https://pola-rs.github.io/polars-book/user-guide/io/multiple/#reading-into-a-single-dataframe) for an example in Python.
:::

#### From an URL

The `read_csv()` method also works with an URL:

```{r}
#| label: read_csv-from-URL-polars
pl$read_csv("https://j.mp/iriscsv")
```

ðŸ‘‰ For a complete list of arguments to use with the `read_csv()` method, see [this page](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.read_csv.html#polars.read_csv).


### Scan a csv file

The `scan_csv()` method can be used to lazily read a csv file from a file.  
`pl$scan_csv()` returns a `LazyFrame`.  
It's argument are the same as `read_csv()` method (see section above).

> This allows the query optimizer to push down predicates and projections to the scan level, thereby potentially reducing memory overhead.

```{r}
#| label: lazy_csv-from-file-polars

pl$scan_csv(
  "examples/iris.csv")$select( # lazy, don't do a thing
    pl$col("Petal.Length","Petal.Width") # select only 2 columns
  )$
  filter(
    pl$col("Petal.Length") > 4 # the filter is pushed down the scan, so less data is read into memory
  )$collect() # <- don't forget collect() here!
```

ðŸ‘‰ For a complete list of arguments to use with the `lazy_csv_reader()` method, see [this page](https://rpolars.github.io/reference/lazy_csv_reader/).

::: {.callout-important}
June 2023: arguments available in Python `eol_char` and `with_column_names` not yet supporting in R
:::


### Scan a parquet file

#### From a single file

The `scan_parquet()` method can be used to lazily read a parquet file from a file.  
Scanning delays the actual parsing of the file and `pl$scan_parquet()` returns a `LazyFrame`.  

Its main arguments are:  

- `path`: path to file.  
- `n_rows`: integer. Limit rows to scan.  
- `cache`: boolean. Cache the result.  
- `parallel`: string. Either "Auto", "None", "Columns" or "RowGroups". The way to parallelized the scan.  
- `rechunk`: boolean. `rechunk` reorganize memory layout, potentially make future operations faster , however perform reallocation now.  
- `row_count_name`: string. Name of a added row count column.  
- `row_count_offset`: integer. Offset to start the row_count column (only used if the name is set).  
- `low_memory`: boolean. Reduce memory usage in expense of performance.  

```{r}
#| label: scan-parquet-polars
pl$scan_parquet("examples/iris.parquet")
```

ðŸ‘‰ For a complete list of arguments to use with the `scan_parquet()` method, see [this page](https://rpolars.github.io/reference/scan_parquet/).

At the end of the query, don't forget to use the `collect()` method to inform Polars that you want to execute it.

```{r}
#| label: scan-parquet-collect-polars
pl$scan_parquet("examples/iris.parquet")$
  collect()
```

::: {.callout-caution}
August 2023 : Export methods have not yet been implemented in R. This methods start with `write_` (write_parquet(), write_parquet(), write_json(), write_ndjson()...)
:::


#### From multiple files

The `scan_parquet()` method can also be used to lazily read multiple parquet files in the same folder.  
This is particularly useful for partitioned files!
For example:

```{r}
#| label: scan-parquet-multiple-polars
# Write multiple parquet files in examples folder
arrow::write_dataset(dataset = iris,
                     path = "examples",
                     partitioning = "Species")
# Reading all parquet files in the example folder and its subfolders
pl$scan_parquet("examples/*/*.parquet")$
  collect()
```

In the code above:  

- `/*` refers to all subfolders in the `example` folder  
- `/*.parquet` refers to all files with a `.parquet` extension  

::: {.callout-important}
In this case, note that the `Species` column which been used for partitioning **is missing**
:::

Of course, the advantage of using `pl$scan_parquet()` is that you can query several partitioned files and retrieve the result of the query in R. See an example [here](#from-a-parquet-file).